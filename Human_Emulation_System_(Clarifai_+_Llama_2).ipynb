{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMnV43PFMZAZhxfi75Q+cqw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mind-Interfaces/Human-Emulation-System/blob/main/Human_Emulation_System_(Clarifai_%2B_Llama_2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "import os\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    import clarifai_grpc\n",
        "    print ('GRPC already installed.')\n",
        "except ImportError:\n",
        "    os.system('pip install clarifai-grpc')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lC0t-kNKrBwk",
        "outputId": "68fe07b3-0e3d-496d-a978-442ff6c82638"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GRPC already installed.\n",
            "CPU times: user 1.53 ms, sys: 0 ns, total: 1.53 ms\n",
            "Wall time: 1.54 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "try:\n",
        "    import gradio as gr\n",
        "    print ('Gradio already installed.')\n",
        "except ImportError:\n",
        "    os.system('pip install gradio')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOUvtR13rc_Q",
        "outputId": "c8126e85-1a92-4125-9653-7baf3aee878f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradio already installed.\n",
            "CPU times: user 1.99 ms, sys: 2 µs, total: 1.99 ms\n",
            "Wall time: 2.06 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1QcZHqdqaQy",
        "outputId": "c40a0295-b08b-47e1-a3f0-eee7595f67ae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 33 µs, sys: 4 µs, total: 37 µs\n",
            "Wall time: 41 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Human Emulation System (Edge Edition) (HES-EDGE.py)\n",
        "\n",
        "# Import Gradio\n",
        "from gradio import Interface\n",
        "from gradio.components import Textbox\n",
        "from clarifai_grpc.grpc.api import resources_pb2, service_pb2, service_pb2_grpc\n",
        "from clarifai_grpc.grpc.api.status import status_code_pb2\n",
        "from clarifai_grpc.channel.clarifai_channel import ClarifaiChannel"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class HumanEmulationSystem:\n",
        "    def __init__(self):\n",
        "        # Define configuration settings.\n",
        "        self.clarifai_pat = 'YOUR_PAT_HERE' # API key for Clarifai.\n",
        "\n",
        "        # Configure log.\n",
        "        self.chat_history = \"\"\n",
        "\n",
        "        # Initialize Clarifai API.\n",
        "        self.channel = ClarifaiChannel.get_grpc_channel()\n",
        "        self.stub = service_pb2_grpc.V2Stub(self.channel)\n",
        "        self.metadata = (('authorization', 'Key ' + self.clarifai_pat),)\n",
        "\n",
        "    def get_clarifai_response(self, model_id, user_id, app_id, text_input):\n",
        "        \"\"\"Fetch response from Clarifai model.\"\"\"\n",
        "        try:\n",
        "            user_data_object = resources_pb2.UserAppIDSet(user_id=user_id, app_id=app_id)\n",
        "            post_model_outputs_response = self.stub.PostModelOutputs(\n",
        "                service_pb2.PostModelOutputsRequest(\n",
        "                    user_app_id=user_data_object,\n",
        "                    model_id=model_id,\n",
        "                    inputs=[\n",
        "                        resources_pb2.Input(\n",
        "                            data=resources_pb2.Data(\n",
        "                                text=resources_pb2.Text(\n",
        "                                    raw=text_input\n",
        "                                )\n",
        "                            )\n",
        "                        )\n",
        "                    ]\n",
        "                ),\n",
        "                metadata=self.metadata\n",
        "            )\n",
        "            if post_model_outputs_response.status.code != status_code_pb2.SUCCESS:\n",
        "                raise Exception(f\"Post model outputs failed, status: {post_model_outputs_response.status.description}\")\n",
        "\n",
        "            return post_model_outputs_response.outputs[0].data.text.raw\n",
        "        except Exception as e:\n",
        "            return None\n",
        "\n",
        "    def generate_response(self, text_input):\n",
        "        \"\"\"Generate a text response.\"\"\"\n",
        "        # Use CodeLlama as logic model.\n",
        "        logic_response = self.get_clarifai_response('CodeLlama-7B-Instruct-GPTQ', 'clarifai', 'ml', text_input)\n",
        "\n",
        "        # Use Orca as reason model.\n",
        "        reason_response = self.get_clarifai_response('orca_mini_v3_13B-GPTQ', 'clarifai', 'ml', text_input)\n",
        "\n",
        "        # META to Combine responses.\n",
        "        combined_response = f\"(Logical Response: {logic_response})\\n(Creative Response: {reason_response})\"\n",
        "        combined_input = f\"Instruction: {text_input}]\\n[Integrate both of the responses into a unified response:\"\n",
        "        combined_input += f\"\\n{combined_response}]\\n[Combine the two responses into a single, cohesive response:]\\n\"\n",
        "\n",
        "        # Use Llama chat model to recompile response.\n",
        "        # response = self.get_clarifai_response('llama2-70b-chat', 'meta', 'Llama-2', combined_input)\n",
        "        response = self.get_clarifai_response('llama2-13b-chat', 'meta', 'Llama-2', combined_input)\n",
        "\n",
        "        return response"
      ],
      "metadata": {
        "id": "yTruB5SAqvOf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the Human Emulation System.\n",
        "HES = HumanEmulationSystem()\n",
        "\n",
        "# Create and launch Gradio Interface.\n",
        "interface = Interface(\n",
        "    fn=HES.generate_response,\n",
        "    inputs=Textbox(lines=10, placeholder=\"Enter Text Here...\"),\n",
        "    outputs=\"text\"\n",
        ")\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    interface.launch(share=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 590
        },
        "id": "z5VLreCJqz6j",
        "outputId": "c65fcabf-bd48-46be-b4e7-37f2bf09a20c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://6562cbe7a818363de4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://6562cbe7a818363de4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}